\documentclass[english]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{soul}
\usepackage{seqsplit}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\begin{document}

\section*{Task 5.1, a}
The learning approach described in the article uses deep learning in convolutional neural net, called Q-network, for developing optimal $Q^{*}(s,a)$ action-value function for playing set of computer games in Atari 2600. 
\\Two key ideas were introduced to update Q-learning in order to use with neural network: first one is using the pool of stored samples to take random and perform "replay" on them, second one is periodically updating target values, not always. Both are for reducing correlations: in observations and in targets. For updating was used loss function $$L_{i}(\theta_{i})=E_{(s,a,r,s^{\prime})\approx U(D)}[(r+\gamma max_{a^{\prime}} Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]$$
Here $\theta_{i}$ are the parameters (weights of the neural net) on the $i-th$ step, $\gamma$ is a discount factor (that was taken 0.99) for learning, $\theta_{i}^{-}$ the parameters that are used for obtaining target values on the $i-th$ step (they are updated every $C$ steps), $U(D)$ is a batch of experience drawn uniformly from stored experiences ( tuples $(s_{t},a_{t},r_{t},s_{t+1})$ ). 
\\The input for the convolutional neural net was $84\times84\times4$ image, preprocessed from the raw input from Atari 2600 and the output is a vector of $Q-values$ for each of the possible actions. Convolutional layers of Q-network here played role of feature extractor and were used for dimension reduction: instead of $4\times 84\times 84$ input nodes we got $32\times 9\times 9$ input nodes. For different games were set different sets of actions. Also, the rewards in the game were simplified to 1, 0 and -1, as the scaling was different in every game. For generating next action was used greedy policy, action is given to the framework Atari 2600 and next image and reward are returned. 
\\In general algorithm worked in the following sequence:
\begin{enumerate}
\item Make an action following greedy policy
\item Get the reward and next state, save them to the pool
\item Sample transitions from the pool and set the max value to calculate loss function
\item Perform gradient descent step
\item Every C steps update weights for generating max Q values
\end{enumerate}
For testing the results the greedy policy with $\epsilon = 0.05$ was also used in order to minimise the possibility of overfitting.

\section*{Task 5.1, b}
States are the frames from Atari 2600, simplified slightly in order to reduce the memory and processing consumptions. So it is $84\times84\times4$ image of the current positions in the game. 
\\Actions are possible actions in the specific game. They are set as initial values for the neural net, and represented by the output layer. So each output in the net is giving Q-value for the corresponding action.

\section*{Task 5.1, c}
Parameters that are adjusted by learning are the weights $\theta_{i}$ of the convolutional neural net (Q-network) at $i-th$ iteration. Number of the parameters (weigths): $8\times 8\times 4 \times 16$ ( 1st hidden layer has 16 filters, and each contains four $8\times 8$ weight matrixes - as we have four, not one frame as input ) + $4\times 4\times 16 \times 32$ ( 2nd hidden layer: 32 filters, each has sixteen $4\times 4$ matrixes, for each input (filter from previous layer) ) + $9\times 9\times 32\times 256$ ( 3rd hidden, that consist of 256 units, each has thirty two $9\times 9$ matrix - 32 - number of inputs from layer before ) + $256\times ActionsNumber$ ( output, $ActionsNumber$ is number of actions for the specific game ).

\section*{Task 5.2}
\cite{deep-learning}
Deep learning with neural network of form 80-40-20 was used (after experiments with other formations). The notation 80-40-20 means that the stacked autoencoders consist of three layers, where
the first one receives the 175-dimensional input and computes an 80-dimensional hidden representation which is transformed into a 40-dimensional representation in the second layer until finally the third layer returns a 20-dimensional feature representation. So the number of the adjusted parameters is $175\times 80 + 80 \times 40 + 40 \times 20$.

\begin{thebibliography}{9}

\bibitem{deep-learning}
  \emph{Deep Learning for Reinforcement Learning in Pacman},
   http://tuprints.ulb.tu-darmstadt.de/id/eprint/3832,
  Bachelor-Thesis von Aaron Hochlï¿½nder aus Wiesbaden
  Juli 2014.

\end{thebibliography}

\end{document}